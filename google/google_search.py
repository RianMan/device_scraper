#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Googleæœç´¢æ¨¡å— - google/google_search.py
è´Ÿè´£é€šè¿‡Googleæœç´¢æŸ¥æ‰¾GSMArenaé“¾æ¥
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import logging
from urllib.parse import quote_plus, urljoin
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException

logger = logging.getLogger(__name__)

class GoogleSearcher:
    def __init__(self, request_delay=2):
        """åˆå§‹åŒ–Googleæœç´¢å™¨"""
        self.request_delay = request_delay
        self.session = requests.Session()
        
        # éšæœºUser-Agentæ± 
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        
        self._update_session_headers()
        self.driver = None
        self._init_driver()
    
    def _get_random_user_agent(self):
        """è·å–éšæœºUser-Agent"""
        return random.choice(self.user_agents)
    
    def _update_session_headers(self):
        """æ›´æ–°è¯·æ±‚å¤´"""
        self.session.headers.update({
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'DNT': '1',
        })
    
    def _init_driver(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument(f'--user-agent={self._get_random_user_agent()}')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(30)
            logger.info("Googleæœç´¢WebDriveråˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.driver = None
    
    def _random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(self.request_delay * 0.8, self.request_delay * 1.5)
        time.sleep(delay)
    
    def search_gsmarena_links(self, model_code):
        """é€šè¿‡Googleæœç´¢æŸ¥æ‰¾GSMArenaé“¾æ¥"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = f"{model_code} site:gsmarena.com"
            encoded_query = quote_plus(search_query)
            google_url = f"https://www.google.com/search?q={encoded_query}"
            
            logger.info(f"ğŸ” Googleæœç´¢: {search_query}")
            
            if not self.driver:
                logger.error("WebDriveræœªåˆå§‹åŒ–")
                return []
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            self._random_delay()
            
            # è®¿é—®Googleæœç´¢é¡µé¢
            self.driver.get(google_url)
            
            # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
            wait = WebDriverWait(self.driver, 15)
            try:
                # ç­‰å¾…æœç´¢ç»“æœå®¹å™¨
                search_container = wait.until(
                    EC.presence_of_element_located((By.ID, "search"))
                )
                
                # ç­‰å¾…ç»“æœåˆ—è¡¨
                results_container = wait.until(
                    EC.presence_of_element_located((By.ID, "rso"))
                )
                
                time.sleep(2)  # ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                
            except TimeoutException:
                logger.warning(f"Googleæœç´¢é¡µé¢åŠ è½½è¶…æ—¶: {model_code}")
                return []
            
            # è§£ææœç´¢ç»“æœ
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            gsmarena_links = self._extract_gsmarena_links(soup, model_code)
            
            return gsmarena_links
            
        except Exception as e:
            logger.error(f"Googleæœç´¢å¤±è´¥ {model_code}: {str(e)}")
            return []
    
    def _extract_gsmarena_links(self, soup, model_code):
        """ä»Googleæœç´¢ç»“æœä¸­æå–GSMArenaé“¾æ¥"""
        gsmarena_links = []
        
        try:
            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_div = soup.find('div', {'id': 'search'})
            if not search_div:
                logger.warning(f"æœªæ‰¾åˆ°Googleæœç´¢ç»“æœå®¹å™¨: {model_code}")
                return []
            
            # æŸ¥æ‰¾ç»“æœåˆ—è¡¨å®¹å™¨
            rso_div = search_div.find('div', {'id': 'rso'})
            if not rso_div:
                logger.warning(f"æœªæ‰¾åˆ°Googleç»“æœåˆ—è¡¨: {model_code}")
                return []
            
            # æŸ¥æ‰¾æ‰€æœ‰ç»“æœé¡¹
            result_items = rso_div.find_all('div', class_='MjjYud')
            logger.info(f"æ‰¾åˆ° {len(result_items)} ä¸ªGoogleæœç´¢ç»“æœ")
            
            for i, item in enumerate(result_items):
                try:
                    # æŸ¥æ‰¾é“¾æ¥
                    link_element = item.find('a', href=True)
                    if not link_element:
                        continue
                    
                    href = link_element.get('href', '')
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯GSMArenaé“¾æ¥
                    if 'gsmarena.com' in href and href.startswith('http'):
                        # æå–æ ‡é¢˜
                        title_element = link_element.find('h3')
                        title = title_element.get_text(strip=True) if title_element else ''
                        
                        # æå–æè¿°
                        description = ''
                        desc_div = item.find('div', class_='VwiC3b')
                        if desc_div:
                            description = desc_div.get_text(strip=True)
                        
                        gsmarena_link = {
                            'url': href,
                            'title': title,
                            'description': description,
                            'rank': i + 1
                        }
                        
                        gsmarena_links.append(gsmarena_link)
                        logger.info(f"âœ… æ‰¾åˆ°GSMArenaé“¾æ¥ #{i+1}: {title}")
                        
                except Exception as e:
                    logger.warning(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {str(e)}")
                    continue
            
            if gsmarena_links:
                logger.info(f"ğŸ¯ Googleæœç´¢æ‰¾åˆ° {len(gsmarena_links)} ä¸ªGSMArenaé“¾æ¥")
            else:
                logger.warning(f"âŒ Googleæœç´¢æœªæ‰¾åˆ°GSMArenaé“¾æ¥: {model_code}")
            
            return gsmarena_links
            
        except Exception as e:
            logger.error(f"æå–GSMArenaé“¾æ¥å¤±è´¥: {str(e)}")
            return []
    
    def search_general_info(self, model_code):
        """é€šç”¨Googleæœç´¢ï¼ˆä¸é™åˆ¶siteï¼‰"""
        try:
            search_query = model_code
            encoded_query = quote_plus(search_query)
            google_url = f"https://www.google.com/search?q={encoded_query}"
            
            logger.info(f"ğŸ” Googleé€šç”¨æœç´¢: {search_query}")
            
            if not self.driver:
                logger.error("WebDriveræœªåˆå§‹åŒ–")
                return []
            
            self._random_delay()
            
            self.driver.get(google_url)
            
            wait = WebDriverWait(self.driver, 15)
            try:
                search_container = wait.until(
                    EC.presence_of_element_located((By.ID, "search"))
                )
                time.sleep(2)
                
            except TimeoutException:
                logger.warning(f"é€šç”¨æœç´¢é¡µé¢åŠ è½½è¶…æ—¶: {model_code}")
                return []
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æå–æ‰€æœ‰ç›¸å…³é“¾æ¥ï¼ˆåŒ…æ‹¬GSMArenaå’Œå…¶ä»–ç«™ç‚¹ï¼‰
            all_links = self._extract_all_relevant_links(soup, model_code)
            
            return all_links
            
        except Exception as e:
            logger.error(f"é€šç”¨Googleæœç´¢å¤±è´¥ {model_code}: {str(e)}")
            return []
    
    def _extract_all_relevant_links(self, soup, model_code):
        """æå–æ‰€æœ‰ç›¸å…³é“¾æ¥"""
        relevant_links = []
        
        try:
            search_div = soup.find('div', {'id': 'search'})
            if not search_div:
                return []
            
            rso_div = search_div.find('div', {'id': 'rso'})
            if not rso_div:
                return []
            
            result_items = rso_div.find_all('div', class_='MjjYud')
            
            for i, item in enumerate(result_items):
                try:
                    link_element = item.find('a', href=True)
                    if not link_element:
                        continue
                    
                    href = link_element.get('href', '')
                    if not href.startswith('http'):
                        continue
                    
                    title_element = link_element.find('h3')
                    title = title_element.get_text(strip=True) if title_element else ''
                    
                    # è¯†åˆ«ç«™ç‚¹ç±»å‹
                    site_type = 'other'
                    if 'gsmarena.com' in href:
                        site_type = 'gsmarena'
                    elif 'gsmchoice.com' in href:
                        site_type = 'gsmchoice'
                    elif any(word in href.lower() for word in ['phone', 'mobile', 'spec', 'review']):
                        site_type = 'mobile_related'
                    
                    link_info = {
                        'url': href,
                        'title': title,
                        'site_type': site_type,
                        'rank': i + 1
                    }
                    
                    relevant_links.append(link_info)
                    
                except Exception as e:
                    continue
            
            logger.info(f"é€šç”¨æœç´¢æ‰¾åˆ° {len(relevant_links)} ä¸ªç›¸å…³é“¾æ¥")
            return relevant_links
            
        except Exception as e:
            logger.error(f"æå–ç›¸å…³é“¾æ¥å¤±è´¥: {str(e)}")
            return []
    
    def close(self):
        """å…³é—­WebDriver"""
        if self.driver:
            self.driver.quit()
            logger.info("Googleæœç´¢WebDriverå·²å…³é—­")