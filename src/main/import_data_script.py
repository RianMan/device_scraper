# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹çš„æ•°æ®å¯¼å…¥è„šæœ¬ - é¿å…Flaskè·¯ç”±å†²çª
"""

import pandas as pd
import pymongo
from pymongo import MongoClient
import json
import logging
import time
from datetime import datetime
import os
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥çˆ¬è™«æ¨¡å—ï¼ˆåªå¯¼å…¥çˆ¬è™«ç±»ï¼Œä¸å¯¼å…¥Flaskåº”ç”¨ï¼‰
from device_scraper_core import DeviceInfoScraper

class DataImporter:
    def __init__(self, mongo_uri="mongodb://localhost:27017/", db_name="device_info", max_workers=5):
        """åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨"""
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.max_workers = max_workers
        self.client = None
        self.db = None
        self.collection = None
        
        # åˆå§‹åŒ–çˆ¬è™«ï¼ˆæ”¯æŒå¤šçº¿ç¨‹ï¼Œ5ç§’é—´éš”ï¼‰
        self.scraper = DeviceInfoScraper(max_workers=max_workers, timeout=60, request_delay=5)
        
        # åˆå§‹åŒ–MongoDBè¿æ¥
        self._init_mongodb()
    
    def _init_mongodb(self):
        """åˆå§‹åŒ–MongoDBè¿æ¥"""
        try:
            self.client = MongoClient(self.mongo_uri)
            self.db = self.client[self.db_name]
            self.collection = self.db['devices']
            
            # åˆ›å»ºç´¢å¼•
            self.collection.create_index("model_code", unique=True)
            self.collection.create_index("device_name")
            
            logger.info(f"MongoDBè¿æ¥æˆåŠŸ: {self.db_name}")
        except Exception as e:
            logger.error(f"MongoDBè¿æ¥å¤±è´¥: {str(e)}")
            raise
    
    def read_csv_data(self, csv_file="device_result.csv"):
        """è¯»å–CSVæ–‡ä»¶ä¸­çš„è®¾å¤‡æ•°æ®"""
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file)
            logger.info(f"æˆåŠŸè¯»å–CSVæ–‡ä»¶: {csv_file}, å…± {len(df)} è¡Œæ•°æ®")
            
            # æå–è®¾å¤‡ä¿¡æ¯
            devices = []
            for index, row in df.iterrows():
                # è§£æclientmanufactureå’Œclientmodel
                manufacture = str(row['clientmanufacture']).strip() if pd.notna(row['clientmanufacture']) else ''
                model = str(row['clientmodel']).strip() if pd.notna(row['clientmodel']) else ''
                
                if manufacture and model:
                    devices.append({
                        'manufacture': manufacture,
                        'model_code': model,
                        'raw_data': f"{manufacture} {model}"
                    })
            
            logger.info(f"æå–åˆ° {len(devices)} ä¸ªæœ‰æ•ˆè®¾å¤‡ä¿¡æ¯")
            return devices
            
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def normalize_model_code(self, model_code):
        """æ ‡å‡†åŒ–è®¾å¤‡å‹å·ä»£ç ï¼Œç”¨äºæ•°æ®åº“åŒ¹é…"""
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å¹¶æ ‡å‡†åŒ–
        normalized = ' '.join(model_code.strip().split())
        return normalized
    
    def filter_existing_devices(self, devices):
        """è¿‡æ»¤æ‰æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è®¾å¤‡ï¼ˆè€ƒè™‘å‹å·æ ‡å‡†åŒ–ï¼‰"""
        existing_codes = set()
        try:
            # è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„å‹å·
            cursor = self.collection.find({}, {"model_code": 1})
            for doc in cursor:
                # æ ‡å‡†åŒ–å·²å­˜åœ¨çš„å‹å·ä»£ç 
                normalized_code = self.normalize_model_code(doc["model_code"])
                existing_codes.add(normalized_code)
            
            logger.info(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(existing_codes)} ä¸ªè®¾å¤‡")
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢å·²å­˜åœ¨è®¾å¤‡å¤±è´¥: {str(e)}")
        
        # è¿‡æ»¤æ–°è®¾å¤‡
        new_devices = []
        for device in devices:
            normalized_code = self.normalize_model_code(device['model_code'])
            if normalized_code not in existing_codes:
                new_devices.append(device)
            else:
                logger.info(f"è®¾å¤‡å·²å­˜åœ¨ï¼Œè·³è¿‡: {device['model_code']} (æ ‡å‡†åŒ–: {normalized_code})")
        
        logger.info(f"éœ€è¦å¤„ç†çš„æ–°è®¾å¤‡: {len(new_devices)} ä¸ª")
        return new_devices
    
    def store_device_batch(self, scrape_results):
        """æ‰¹é‡å­˜å‚¨è®¾å¤‡ä¿¡æ¯åˆ°æ•°æ®åº“"""
        success_count = 0
        error_count = 0
        
        for result_data in scrape_results:
            try:
                device_info = result_data['device_info']
                scrape_result = result_data['scrape_result']
                data = scrape_result['data']
                
                # æ„å»ºè¦å­˜å‚¨çš„æ–‡æ¡£
                device_doc = {
                    "model_code": device_info['model_code'],
                    "device_name": data['device_name'],
                    "announced_date": data['announced_date'],
                    "release_date": data['release_date'],
                    "price": data['price'],  # ç›´æ¥å­˜å‚¨åŸå§‹ä»·æ ¼å­—ç¬¦ä¸²
                    "manufacture": data['manufacture'],
                    "source_url": data['source_url'],
                    "created_at": datetime.now(),
                    "updated_at": datetime.now(),
                    "specifications": data['specifications']  # å®Œæ•´è§„æ ¼ä¿¡æ¯
                }
                
                # æ’å…¥æ•°æ®åº“
                self.collection.insert_one(device_doc)
                success_count += 1
                
            except Exception as e:
                logger.error(f"å­˜å‚¨è®¾å¤‡ {device_info['model_code']} å¤±è´¥: {str(e)}")
                error_count += 1
        
        logger.info(f"æ‰¹é‡å­˜å‚¨å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")
        return success_count, error_count
    
    def progress_callback(self, completed, total, success, failed):
        """è¿›åº¦å›è°ƒå‡½æ•°"""
        progress = completed / total * 100
        print(f"\rğŸ”„ è¿›åº¦: {completed}/{total} ({progress:.1f}%) | æˆåŠŸ: {success} | å¤±è´¥: {failed}", end='', flush=True)
    
    def batch_process_devices(self, csv_file="device_result.csv"):
        """æ‰¹é‡å¹¶è¡Œå¤„ç†è®¾å¤‡ä¿¡æ¯"""
        # è¯»å–CSVæ•°æ®
        devices = self.read_csv_data(csv_file)
        
        if not devices:
            logger.error("æ²¡æœ‰è¯»å–åˆ°è®¾å¤‡æ•°æ®")
            return
        
        # è¿‡æ»¤å·²å­˜åœ¨çš„è®¾å¤‡
        new_devices = self.filter_existing_devices(devices)
        
        if not new_devices:
            logger.info("æ‰€æœ‰è®¾å¤‡éƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­")
            return
        
        logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(new_devices)} ä¸ªæ–°è®¾å¤‡ï¼Œçº¿ç¨‹æ•°: {self.max_workers}")
        
        # å¹¶è¡Œçˆ¬å–è®¾å¤‡ä¿¡æ¯
        scrape_results, failed_devices = self.scraper.batch_get_device_info(
            new_devices, 
            progress_callback=self.progress_callback
        )
        
        print()  # æ¢è¡Œ
        
        # æ‰¹é‡å­˜å‚¨æˆåŠŸçš„ç»“æœ
        if scrape_results:
            store_success, store_error = self.store_device_batch(scrape_results)
        else:
            store_success, store_error = 0, 0
        
        # ä¿å­˜å¤±è´¥çš„è®¾å¤‡ä¿¡æ¯
        self.save_failed_devices(failed_devices)
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        total_count = len(new_devices)
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ!")
        logger.info(f"æ€»æ•°: {total_count}")
        logger.info(f"çˆ¬å–æˆåŠŸ: {len(scrape_results)}")
        logger.info(f"å­˜å‚¨æˆåŠŸ: {store_success}")
        logger.info(f"çˆ¬å–å¤±è´¥: {len(failed_devices)}")
        logger.info(f"æˆåŠŸç‡: {len(scrape_results)/total_count*100:.1f}%")
    
    def save_failed_devices(self, failed_devices):
        """ä¿å­˜æŸ¥è¯¢å¤±è´¥çš„è®¾å¤‡ä¿¡æ¯"""
        if not failed_devices:
            logger.info("æ²¡æœ‰å¤±è´¥çš„è®¾å¤‡éœ€è¦ä¿å­˜")
            return
        
        # ä¿å­˜ä¸ºCSV
        failed_df = pd.DataFrame(failed_devices)
        csv_filename = f"failed_devices_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        failed_df.to_csv(csv_filename, index=False)
        
        # ä¿å­˜ä¸ºJSON
        json_filename = f"failed_devices_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(failed_devices, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤±è´¥è®¾å¤‡ä¿¡æ¯å·²ä¿å­˜:")
        logger.info(f"CSVæ–‡ä»¶: {csv_filename}")
        logger.info(f"JSONæ–‡ä»¶: {json_filename}")
    
    def get_stats(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            total_count = self.collection.count_documents({})
            with_price = self.collection.count_documents({"price": {"$ne": ""}})
            with_date = self.collection.count_documents({"announced_date": {"$ne": ""}})
            
            stats = {
                "total_devices": total_count,
                "devices_with_price": with_price,
                "devices_with_date": with_date,
                "price_coverage": f"{with_price/total_count*100:.1f}%" if total_count > 0 else "0%",
                "date_coverage": f"{with_date/total_count*100:.1f}%" if total_count > 0 else "0%"
            }
            
            return stats
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.scraper:
            self.scraper.close()
        if self.client:
            self.client.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    csv_file = "device_result.csv"
    if not os.path.exists(csv_file):
        logger.error(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        logger.info("è¯·ç¡®ä¿CSVæ–‡ä»¶å­˜åœ¨å¹¶åŒ…å« 'clientmanufacture' å’Œ 'clientmodel' åˆ—")
        return
    
    # åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨ï¼ˆ5ä¸ªå¹¶å‘çº¿ç¨‹ï¼Œ5ç§’é—´éš”ï¼‰
    try:
        importer = DataImporter(max_workers=5)
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨å¤±è´¥: {str(e)}")
        logger.info("è¯·ç¡®ä¿MongoDBæœåŠ¡å·²å¯åŠ¨")
        return
    
    try:
        # å¼€å§‹æ‰¹é‡å¹¶è¡Œå¤„ç†
        start_time = time.time()
        logger.info("ğŸš€ å¼€å§‹å¤„ç†è®¾å¤‡æ•°æ®...")
        logger.info("âš™ï¸  é…ç½®: 5ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªè¯·æ±‚é—´éš”5ç§’")
        
        importer.batch_process_devices(csv_file)
        end_time = time.time()
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = importer.get_stats()
        logger.info("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in stats.items():
            logger.info(f"  {key}: {value}")
        
        logger.info(f"\nâ±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
    finally:
        # å…³é—­è¿æ¥
        importer.close()

if __name__ == "__main__":
    main()