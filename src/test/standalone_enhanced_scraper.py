#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹çš„å¢å¼ºç‰ˆGSMChoiceçˆ¬è™« - åŒ…å«æµ‹è¯•ä»£ç 
"""

import requests
from bs4 import BeautifulSoup
import json
import logging
import time
from datetime import datetime
import os
import re
from urllib.parse import quote_plus
import sys

# å°è¯•å¯¼å…¥Seleniumï¼ˆå¯é€‰ï¼‰
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.common.exceptions import TimeoutException, WebDriverException
    SELENIUM_AVAILABLE = True
    print("âœ… Seleniumå¯ç”¨")
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸  Seleniumä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨requestsæ¨¡å¼")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedGSMChoiceScraper:
    def __init__(self, request_delay=3, use_selenium=True):
        """åˆå§‹åŒ–å¢å¼ºçš„GSMChoiceçˆ¬è™«"""
        self.base_url = "https://www.gsmchoice.com"
        self.search_api = "https://www.gsmchoice.com/js/searchy.xhtml"
        self.request_delay = request_delay
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        
        # åˆå§‹åŒ–requests session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.gsmchoice.com/en/',
        })
        
        # åˆå§‹åŒ–Selenium WebDriverï¼ˆå¦‚æœå¯ç”¨ä¸”éœ€è¦ï¼‰
        self.driver = None
        if self.use_selenium:
            self._init_driver()
        else:
            logger.info("ä½¿ç”¨requestsæ¨¡å¼ï¼ˆæ— JavaScriptæ”¯æŒï¼‰")
    
    def _init_driver(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        if not SELENIUM_AVAILABLE:
            logger.warning("Seleniumä¸å¯ç”¨ï¼Œè·³è¿‡WebDriveråˆå§‹åŒ–")
            return
            
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(30)
            logger.info("âœ… Selenium WebDriveråˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.driver = None
            self.use_selenium = False
    
    def search_device(self, manufacture, model):
        """æœç´¢è®¾å¤‡ï¼ˆå…ˆç”¨APIï¼Œå¤±è´¥åˆ™ç”¨ç½‘é¡µæœç´¢ï¼‰"""
        # æ–¹æ³•1ï¼šå°è¯•æœç´¢API
        api_result = self._search_via_api(manufacture, model)
        if api_result:
            return api_result
        
        # æ–¹æ³•2ï¼šç½‘é¡µæœç´¢
        logger.info("APIæœç´¢å¤±è´¥ï¼Œå°è¯•ç½‘é¡µæœç´¢...")
        return self._search_via_web(manufacture, model)
    
    def _search_via_api(self, manufacture, model):
        """é€šè¿‡APIæœç´¢è®¾å¤‡"""
        try:
            search_query = f"{manufacture} {model}"
            encoded_query = quote_plus(search_query)
            
            search_url = f"{self.search_api}?search={encoded_query}&lang=en&v=3"
            logger.info(f"ğŸ” APIæœç´¢è®¾å¤‡: {search_query}")
            
            time.sleep(self.request_delay)
            
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            try:
                results = response.json()
            except json.JSONDecodeError:
                logger.warning(f"æ— æ³•è§£æJSONå“åº”: {manufacture} {model}")
                return None
            
            if not results or len(results) == 0:
                logger.warning(f"APIæœªæ‰¾åˆ°æœç´¢ç»“æœ: {manufacture} {model}")
                return None
            
            first_result = results[0]
            logger.info(f"âœ… APIæ‰¾åˆ°è®¾å¤‡: {first_result.get('brand', '')} {first_result.get('model', '')}")
            return first_result
            
        except Exception as e:
            logger.error(f"âŒ APIæœç´¢å¤±è´¥ {manufacture} {model}: {str(e)}")
            return None
    
    def _search_via_web(self, manufacture, model):
        """é€šè¿‡ç½‘é¡µæœç´¢è®¾å¤‡"""
        try:
            search_query = f"{manufacture} {model}"
            encoded_query = quote_plus(search_query)
            
            search_url = f"{self.base_url}/en/search/?sSearch4={encoded_query}"
            logger.info(f"ğŸŒ ç½‘é¡µæœç´¢è®¾å¤‡: {search_query}")
            
            time.sleep(self.request_delay)
            
            if self.driver:
                # ä½¿ç”¨Selenium
                self.driver.get(search_url)
                time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                # ä½¿ç”¨requests
                response = self.session.get(search_url, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            device_links = soup.find_all('a', href=re.compile(r'/en/catalogue/.*/.*/'))
            
            if not device_links:
                logger.warning(f"ç½‘é¡µæœç´¢æœªæ‰¾åˆ°ç»“æœ: {manufacture} {model}")
                return None
            
            # å–ç¬¬ä¸€ä¸ªç»“æœå¹¶æ„é€ è®¾å¤‡ä¿¡æ¯
            first_link = device_links[0]
            href = first_link.get('href', '')
            
            # ä»URLä¸­æå–sbrandå’Œsmodel
            url_parts = href.strip('/').split('/')
            if len(url_parts) >= 4:
                sbrand = url_parts[-2]
                smodel = url_parts[-1]
                
                device_info = {
                    'brand': manufacture,
                    'model': model,
                    'sbrand': sbrand,
                    'smodel': smodel
                }
                
                logger.info(f"âœ… ç½‘é¡µæœç´¢æ‰¾åˆ°: {sbrand}/{smodel}")
                return device_info
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ç½‘é¡µæœç´¢å¤±è´¥ {manufacture} {model}: {str(e)}")
            return None
    
    def get_device_details(self, device_info):
        """è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯ï¼ˆæ”¯æŒSeleniumï¼‰"""
        try:
            sbrand = device_info.get('sbrand', '')
            smodel = device_info.get('smodel', '')
            
            if not sbrand or not smodel:
                logger.warning("ç¼ºå°‘sbrandæˆ–smodelä¿¡æ¯")
                return None
            
            detail_url = f"{self.base_url}/en/catalogue/{sbrand}/{smodel}/"
            logger.info(f"ğŸ“„ è·å–è¯¦æƒ…é¡µ: {detail_url}")
            
            time.sleep(self.request_delay)
            
            if self.driver:
                # ä½¿ç”¨Seleniumè·å–é¡µé¢
                logger.info("ä½¿ç”¨SeleniumåŠ è½½é¡µé¢...")
                self.driver.get(detail_url)
                
                # ç­‰å¾…é¡µé¢åŸºç¡€å†…å®¹åŠ è½½
                try:
                    WebDriverWait(self.driver, 15).until(
                        EC.any_of(
                            EC.presence_of_element_located((By.CLASS_NAME, "PhoneData")),
                            EC.presence_of_element_located((By.TAG_NAME, "table"))
                        )
                    )
                    logger.info("âœ… é¡µé¢åŸºç¡€å†…å®¹å·²åŠ è½½")
                except TimeoutException:
                    logger.warning("âš ï¸ é¡µé¢åŸºç¡€å†…å®¹åŠ è½½è¶…æ—¶")
                
                # é¢å¤–ç­‰å¾…JavaScriptæ‰§è¡Œ
                time.sleep(5)
                
                # å°è¯•ç­‰å¾…ä»·æ ¼ç»„ä»¶åŠ è½½
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "scard-widget-prices__container"))
                    )
                    logger.info("âœ… ä»·æ ¼å®¹å™¨å·²åŠ è½½")
                except TimeoutException:
                    logger.warning("âš ï¸ ä»·æ ¼å®¹å™¨åŠ è½½è¶…æ—¶ï¼Œç»§ç»­å¤„ç†...")
                
                # å°è¯•æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                self.driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(1)
                
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            else:
                # ä½¿ç”¨requests
                logger.info("ä½¿ç”¨requestsè·å–é¡µé¢...")
                response = self.session.get(detail_url, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¿å­˜è°ƒè¯•HTML
            debug_filename = f'{sbrand}_{smodel}_enhanced_soup.html'
            with open(debug_filename, 'w', encoding='utf-8') as f:
                f.write(str(soup))
            logger.info(f"ğŸ“ è°ƒè¯•HTMLå·²ä¿å­˜: {debug_filename}")
            
            # æå–è®¾å¤‡ä¿¡æ¯
            device_details = {
                'device_name': device_info.get('model', 'Unknown'),
                'brand': device_info.get('brand', ''),
                'announced_date': '',
                'price': '',
                'specifications': {},
                'source_url': detail_url
            }
            
            # æ›´æ–°è®¾å¤‡åç§°ï¼ˆä»é¡µé¢æå–ï¼‰
            title_element = soup.find('h1', class_='infoline__title')
            if title_element:
                title_span = title_element.find('span')
                if title_span:
                    device_details['device_name'] = title_span.get_text(strip=True)
                    logger.info(f"ğŸ“± è®¾å¤‡åç§°: {device_details['device_name']}")
            
            # æå–è§„æ ¼ä¿¡æ¯
            self._extract_specifications(soup, device_details)
            
            # å¢å¼ºçš„ä»·æ ¼æå–
            self._extract_price_enhanced(soup, device_details)
            
            return device_details
            
        except Exception as e:
            logger.error(f"âŒ è·å–è®¾å¤‡è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def _extract_specifications(self, soup, device_details):
        """æå–è§„æ ¼ä¿¡æ¯"""
        try:
            specs_count = 0
            
            # æŸ¥æ‰¾è§„æ ¼è¡¨æ ¼
            spec_table = soup.find('table', class_='PhoneData')
            if not spec_table:
                # å°è¯•å…¶ä»–å¯èƒ½çš„è¡¨æ ¼
                spec_tables = soup.find_all('table', {'cellspacing': '0'})
                if spec_tables:
                    spec_table = spec_tables[0]
            
            if spec_table:
                rows = spec_table.find_all('tr')
                for row in rows:
                    th = row.find('th', class_='phoneCategoryName')
                    td = row.find('td', class_='phoneCategoryValue')
                    
                    if th and td:
                        key = th.get_text(strip=True)
                        value = td.get_text(strip=True)
                        
                        # æ¸…ç†å€¼ä¸­çš„å¤šä½™ç©ºç™½
                        value = ' '.join(value.split())
                        
                        # ç‰¹æ®Šå¤„ç†å‘å¸ƒæ—¥æœŸ
                        if 'Announced' in key:
                            device_details['announced_date'] = value
                            logger.info(f"ğŸ“… å‘å¸ƒæ—¥æœŸ: {value}")
                        
                        # å­˜å‚¨åˆ°è§„æ ¼å­—å…¸
                        if key and value:
                            device_details['specifications'][key] = value
                            specs_count += 1
            
            # å¦‚æœæ²¡æ‰¾åˆ°PhoneDataè¡¨æ ¼ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–è§„æ ¼å®¹å™¨
            if specs_count == 0:
                spec_containers = soup.find_all(['div', 'section'], class_=re.compile(r'spec|phone|data', re.I))
                logger.info(f"å°è¯•ä» {len(spec_containers)} ä¸ªå®¹å™¨ä¸­æå–è§„æ ¼...")
                
                for container in spec_containers:
                    # åœ¨å®¹å™¨ä¸­æŸ¥æ‰¾é”®å€¼å¯¹
                    items = container.find_all(['tr', 'li', 'div'])
                    for item in items:
                        text = item.get_text(strip=True)
                        if ':' in text and len(text) < 200:  # é¿å…å¤ªé•¿çš„æ–‡æœ¬
                            parts = text.split(':', 1)
                            if len(parts) == 2:
                                key, value = parts[0].strip(), parts[1].strip()
                                if key and value and len(key) < 50:
                                    device_details['specifications'][key] = value
                                    specs_count += 1
            
            logger.info(f"ğŸ“‹ æå–äº† {specs_count} ä¸ªè§„æ ¼é¡¹")
            
        except Exception as e:
            logger.error(f"âŒ æå–è§„æ ¼ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def _extract_price_enhanced(self, soup, device_details):
        """å¢å¼ºçš„ä»·æ ¼æå–"""
        try:
            price_found = False
            
            # æ–¹æ³•1ï¼šæŸ¥æ‰¾Amazonä»·æ ¼æŒ‰é’®å’Œé“¾æ¥
            amazon_elements = soup.find_all(['a', 'button'], class_=re.compile(r'amazon|price', re.I))
            logger.info(f"ğŸ’° æ‰¾åˆ° {len(amazon_elements)} ä¸ªä»·æ ¼ç›¸å…³å…ƒç´ ")
            
            for element in amazon_elements:
                href = element.get('href', '')
                text = element.get_text(strip=True)
                
                # è·³è¿‡æ˜æ˜¾çš„é…ä»¶é“¾æ¥
                accessory_keywords = ['HandyhÃ¼lle', 'Schutzfolien', 'Powerbank', 'KopfhÃ¶rer', 'USB-Adapter', 'Speicherkarte', 'kaufen']
                if any(keyword.lower() in text.lower() for keyword in accessory_keywords):
                    continue
                
                # æŸ¥æ‰¾ä»·æ ¼æ¨¡å¼
                price_patterns = [
                    r'(\d+[.,]\d+)\s*(â‚¬|EUR)',
                    r'â‚¬\s*(\d+[.,]\d+)',
                    r'(\d+)\s*(â‚¬|EUR)',
                    r'USD\s*(\d+[.,]\d+)',
                    r'\$(\d+[.,]\d+)'
                ]
                
                for pattern in price_patterns:
                    match = re.search(pattern, text)
                    if match:
                        price_value = match.group(1)
                        currency = match.group(2) if len(match.groups()) > 1 else 'â‚¬'
                        device_details['price'] = f"{price_value} {currency}"
                        price_found = True
                        logger.info(f"ğŸ’° ä»å…ƒç´ æ‰¾åˆ°ä»·æ ¼: {device_details['price']}")
                        break
                
                if price_found:
                    break
            
            # æ–¹æ³•2ï¼šæŸ¥æ‰¾ä»·æ ¼å®¹å™¨
            if not price_found:
                price_containers = soup.find_all(['div', 'span', 'p'], class_=re.compile(r'price|cost|scard-widget', re.I))
                logger.info(f"ğŸ” æ£€æŸ¥ {len(price_containers)} ä¸ªä»·æ ¼å®¹å™¨")
                
                for container in price_containers:
                    price_text = container.get_text(strip=True)
                    price_match = re.search(r'(\d+[.,]\d+|\d+)\s*(â‚¬|EUR|Dollar|\$)', price_text)
                    if price_match:
                        device_details['price'] = price_match.group(0)
                        price_found = True
                        logger.info(f"ğŸ’° ä»ä»·æ ¼å®¹å™¨æ‰¾åˆ°ä»·æ ¼: {device_details['price']}")
                        break
            
            # æ–¹æ³•3ï¼šä»JavaScriptä¸­æŸ¥æ‰¾ä»·æ ¼ä¿¡æ¯
            if not price_found:
                scripts = soup.find_all('script')
                logger.info(f"ğŸ” æ£€æŸ¥ {len(scripts)} ä¸ªè„šæœ¬æ ‡ç­¾")
                
                for script in scripts:
                    if script.string:
                        # æŸ¥æ‰¾ä»·æ ¼ç›¸å…³çš„JavaScriptå˜é‡
                        price_patterns = [
                            r'price["\']?\s*[:=]\s*["\']?(\d+[.,]\d+|\d+)\s*(â‚¬|EUR|Dollar|\$)?["\']?',
                            r'amazonKartaWidget.*?(\d+[.,]\d+)',
                            r'cost["\']?\s*[:=]\s*["\']?(\d+[.,]\d+)'
                        ]
                        
                        for pattern in price_patterns:
                            match = re.search(pattern, script.string, re.IGNORECASE)
                            if match:
                                price_value = match.group(1)
                                currency = match.group(2) if len(match.groups()) > 1 and match.group(2) else 'â‚¬'
                                device_details['price'] = f"{price_value} {currency}"
                                price_found = True
                                logger.info(f"ğŸ’° ä»JavaScriptæ‰¾åˆ°ä»·æ ¼: {device_details['price']}")
                                break
                        
                        if price_found:
                            break
            
            # æ–¹æ³•4ï¼šæŸ¥æ‰¾ä»»ä½•åŒ…å«ä»·æ ¼æ¨¡å¼çš„æ–‡æœ¬ï¼ˆæœ€åå°è¯•ï¼‰
            if not price_found:
                all_text = soup.get_text()
                price_patterns = [
                    r'(\d+[.,]\d+)\s*â‚¬',
                    r'â‚¬\s*(\d+[.,]\d+)',
                    r'(\d+[.,]\d+)\s*EUR',
                    r'USD\s*(\d+[.,]\d+)',
                    r'\$(\d+[.,]\d+)'
                ]
                
                for pattern in price_patterns:
                    matches = re.findall(pattern, all_text)
                    if matches:
                        # å–ç¬¬ä¸€ä¸ªåˆç†çš„ä»·æ ¼ï¼ˆé€šå¸¸åœ¨30-5000èŒƒå›´å†…ï¼‰
                        for match in matches:
                            try:
                                price_num = float(match.replace(',', '.'))
                                if 30 <= price_num <= 5000:
                                    device_details['price'] = f"{match} â‚¬"
                                    price_found = True
                                    logger.info(f"ğŸ’° ä»é¡µé¢æ–‡æœ¬æ‰¾åˆ°ä»·æ ¼: {device_details['price']}")
                                    break
                            except ValueError:
                                continue
                    if price_found:
                        break
            
            # è®¾ç½®é»˜è®¤å€¼
            if not price_found:
                # æ£€æŸ¥æ˜¯å¦æœ‰è´­ä¹°é“¾æ¥
                amazon_links = soup.find_all('a', href=re.compile(r'amazon', re.I))
                if amazon_links:
                    device_details['price'] = "Available on Amazon"
                    logger.info("ğŸ›’ æœªæ‰¾åˆ°å…·ä½“ä»·æ ¼ï¼Œä½†æœ‰Amazonè´­ä¹°é“¾æ¥")
                else:
                    device_details['price'] = "Price not available"
                    logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•ä»·æ ¼ä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"âŒ æå–ä»·æ ¼ä¿¡æ¯å¤±è´¥: {str(e)}")
            device_details['price'] = "Price extraction failed"
    
    def get_device_info(self, manufacture, model):
        """è·å–å®Œæ•´è®¾å¤‡ä¿¡æ¯"""
        try:
            search_result = self.search_device(manufacture, model)
            if not search_result:
                return {
                    'success': False,
                    'message': f'æœªæ‰¾åˆ°è®¾å¤‡ {manufacture} {model}'
                }
            
            device_details = self.get_device_details(search_result)
            if not device_details:
                return {
                    'success': False,
                    'message': f'æ— æ³•è·å–è®¾å¤‡è¯¦æƒ… {manufacture} {model}'
                }
            
            return {
                'success': True,
                'source': 'gsmchoice_enhanced',
                'data': device_details
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–è®¾å¤‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'message': f'è·å–è®¾å¤‡ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
            }
    
    def close(self):
        """å…³é—­WebDriver"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸ”’ WebDriverå·²å…³é—­")

def test_enhanced_scraper():
    """æµ‹è¯•å¢å¼ºçš„çˆ¬è™«"""
    print("ğŸ”§ å¢å¼ºç‰ˆGSMChoiceçˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥Seleniumå¯ç”¨æ€§
    if SELENIUM_AVAILABLE:
        print("âœ… Seleniumå¯ç”¨ï¼Œå°†ä½¿ç”¨å®Œæ•´åŠŸèƒ½")
        use_selenium = True
    else:
        print("âš ï¸ Seleniumä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼")
        use_selenium = False
    
    scraper = EnhancedGSMChoiceScraper(request_delay=2, use_selenium=use_selenium)
    
    try:
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            ("lge", "LG-M430"),
            ("Lanix", "Alpha 1R"),
            ("motorola", "moto g play - 2023"),
        ]
        
        for i, (manufacture, model) in enumerate(test_cases, 1):
            print(f"\nğŸ“± æµ‹è¯• {i}/{len(test_cases)}: {manufacture} {model}")
            print("-" * 50)
            
            try:
                result = scraper.get_device_info(manufacture, model)
                
                if result['success']:
                    data = result['data']
                    print(f"âœ… æˆåŠŸæ‰¾åˆ°è®¾å¤‡")
                    print(f"   è®¾å¤‡åç§°: {data['device_name']}")
                    print(f"   å“ç‰Œ: {data['brand']}")
                    print(f"   å‘å¸ƒæ—¥æœŸ: {data['announced_date']}")
                    print(f"   ä»·æ ¼: {data['price']}")
                    print(f"   è§„æ ¼æ•°é‡: {len(data['specifications'])}")
                    print(f"   æ¥æº: {data['source_url']}")
                    
                    # æ˜¾ç¤ºä¸€äº›å…³é”®è§„æ ¼
                    specs = data['specifications']
                    important_keys = ['Display', 'Processor', 'Standard battery', 'Operating system', 'Dimensions', 'Weight']
                    for key in important_keys:
                        if key in specs:
                            print(f"   {key}: {specs[key]}")
                else:
                    print(f"âŒ å¤±è´¥: {result['message']}")
                    
            except Exception as e:
                print(f"âŒ å¼‚å¸¸: {str(e)}")
                import traceback
                traceback.print_exc()
        
        print(f"\nğŸ”š æµ‹è¯•å®Œæˆ")
        
    finally:
        scraper.close()

if __name__ == "__main__":
    test_enhanced_scraper()